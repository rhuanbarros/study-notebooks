{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in Linear Regression\n",
    "\n",
    "* **Definition:** Learn a function h: X → Y where h(x) predicts y.\n",
    "* **Hypothesis:** The function h(⋅).\n",
    "* **Regression:** Predicting a continuous target variable (e.g., house price).\n",
    "* **Classification:** Predicting a discrete target variable (e.g., house vs. apartment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Representation\n",
    "\n",
    "* **Linear Function:**  $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$\n",
    "    * $\\theta_i$: Parameters/weights\n",
    "    * $x_i$: Input features ($x_0 = 1$ for intercept term)\n",
    "* **Vectorized Form:** $h(x) = \\theta^Tx$  ($\\theta$ and $x$ are vectors)\n",
    "    * $n$: Number of input variables (excluding $x_0$)\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Measures the error between predicted and actual values (e.g., Mean Squared Error).\n",
    "\n",
    "* **Least Squares:** $J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "    * Measures how close predictions are to actual values.\n",
    "    * $m$: Number of training examples\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Iterative algorithm to minimize the cost function and find optimal $\\theta$ values.\n",
    "\n",
    "* **Goal:** Minimize $J(\\theta)$ by iteratively updating $\\theta$.\n",
    "* **General Update Rule:** $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$\n",
    "    * $\\alpha$: Learning rate\n",
    "* **Partial Derivative:** $\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
    "* **LMS Update Rule (Single Training Example):** $\\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}$\n",
    "    * Also known as Widrow-Hoff learning rule.\n",
    "    * Update proportional to the error.\n",
    "\n",
    "## Gradient Descent Variants\n",
    "\n",
    "* **Batch Gradient Descent:**\n",
    "    * Updates $\\theta$ after looking at all training examples.\n",
    "    * $\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^{m} (y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}$ (for all $j$)\n",
    "    * Converges to global minimum for linear regression (convex quadratic function).\n",
    "* **Stochastic Gradient Descent:**\n",
    "    * Updates $\\theta$ after each training example.\n",
    "    * $\\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}$ (for all $j$, for each $i$)\n",
    "    * Faster for large datasets, but may oscillate around the minimum.\n",
    "    * Can converge to minimum with decreasing learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Assumptions\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables (predictors) and the dependent variable is linear.\n",
    "\n",
    "2. **Independence**: Observations are independent of each other, meaning the errors (residuals) are not correlated across observations.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals is constant across all levels of the independent variables.\n",
    "\n",
    "4. **Normality of Residuals**: The residuals (differences between observed and predicted values) should be normally distributed.\n",
    "\n",
    "5. **No Multicollinearity**: Independent variables are not highly correlated with each other.\n",
    "\n",
    "6. **No Autocorrelation**: There is no correlation between the residuals over time (important for time series data).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
