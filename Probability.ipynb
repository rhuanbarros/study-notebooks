{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Cheatsheet for Machine Learning\n",
    "\n",
    "## 1.1 Regression Metrics\n",
    "\n",
    "#### 1.1.2 Residuals\n",
    "- **Definition**: The difference between observed and predicted values in regression.\n",
    "- **Key Concepts**:\n",
    "  - **Residuals Plot**: A graph to detect patterns that indicate model inadequacy.\n",
    "  - **Homoscedasticity**: Residuals have constant variance (ideal scenario).\n",
    "  - **Heteroscedasticity**: Residuals have non-constant variance (indicates model problems).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict values\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='--') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Effect Sizes\n",
    "- **Cohenâ€™s d**: Measures the size of the difference between two means.\n",
    "- **R-Squared (Coefficient of Determination)**: Proportion of variance in the dependent variable explained by the independent variables in the model.\n",
    "- **ROC Curve**: Plot showing the true positive rate vs. false positive rate.\n",
    "  - **AUC (Area Under the Curve)**: Measures overall model performance.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score, roc_curve, auc\n",
    "\n",
    "# Assuming you have y_true (true values) and y_pred (predicted probabilities) for a classification problem\n",
    "\n",
    "# R-squared\n",
    "r2 = r2_score(y_true, y_pred) \n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--') \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Probabilistic Theory\n",
    "\n",
    "#### 1.2.1 Probabilistic Independence\n",
    "- **Definition**: Two events are independent if the occurrence of one does not affect the probability of the other.\n",
    "- **Calculation**: \\( P(A \\cap B) = P(A) \\times P(B) \\)\n",
    "\n",
    "#### 1.2.2 Generative and Discriminative Models\n",
    "- **Generative Models**: Learn the joint probability \\( P(X, Y) \\) (e.g., Naive Bayes, Gaussian Mixture Models).\n",
    "- **Discriminative Models**: Learn the conditional probability \\( P(Y | X) \\) (e.g., Logistic Regression, SVM).\n",
    "\n",
    "#### 1.2.3 Conditional Probabilities\n",
    "- **Bayes' Theorem**: \\( P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\)\n",
    "- **Total Probability**: Used to find probabilities of complex events by breaking them down.\n",
    "\n",
    "```python\n",
    "# Example of Bayes' Theorem\n",
    "prior_A = 0.3  # Prior probability of event A\n",
    "likelihood_B_given_A = 0.8  # Likelihood of event B given A\n",
    "likelihood_B_given_not_A = 0.2  # Likelihood of event B given not A\n",
    "\n",
    "# Calculate P(B) using total probability\n",
    "prob_B = (likelihood_B_given_A * prior_A) + (likelihood_B_given_not_A * (1 - prior_A))\n",
    "\n",
    "# Calculate P(A|B) using Bayes' Theorem\n",
    "posterior_A_given_B = (likelihood_B_given_A * prior_A) / prob_B\n",
    "\n",
    "print(f\"P(A|B) = {posterior_A_given_B}\")\n",
    "```\n",
    "\n",
    "#### 1.2.4 Probability Distributions\n",
    "- **Bernoulli Distribution**: Describes outcomes of a binary event.\n",
    "- **Uniform Distribution**: All outcomes have equal probability.\n",
    "- **Poisson Distribution**: Models the number of events occurring within a fixed interval.\n",
    "- **Binomial Distribution**: Models the number of successes in a fixed number of trials.\n",
    "- **Normal Distribution**: Bell-shaped curve, important for central limit theorem.\n",
    "\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Bernoulli Distribution\n",
    "p = 0.6 # Probability of success\n",
    "bernoulli_rv = stats.bernoulli(p)\n",
    "print(f\"Bernoulli P(X=1): {bernoulli_rv.pmf(1)}\")\n",
    "\n",
    "# Normal Distribution\n",
    "mu = 0 # Mean\n",
    "sigma = 1 # Standard deviation\n",
    "normal_rv = stats.norm(mu, sigma)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, normal_rv.pdf(x))\n",
    "plt.title(\"Normal Distribution PDF\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### 1.2.5 Operations on Probabilities\n",
    "- **Union of Events**: \\( P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\)\n",
    "- **Intersection of Events**: \\( P(A \\cap B) = P(A) \\times P(B) \\) (if independent).\n",
    "- **Complement**: \\( P(\\text{not A}) = 1 - P(A) \\)\n",
    "\n",
    "#### 1.2.6 Probability Density Function (PDF)\n",
    "- **Mean**: The expected value of a random variable.\n",
    "- **Variance**: Measure of how much values spread out around the mean.\n",
    "- **Integral for Probability**: \\( P(a \\leq X \\leq b) = \\int_a^b f(x) dx \\)\n",
    "\n",
    "```python\n",
    "# Example: Calculating probability from PDF of a Normal Distribution\n",
    "mu = 0 \n",
    "sigma = 1\n",
    "normal_rv = stats.norm(mu, sigma)\n",
    "\n",
    "# Probability that X is between -1 and 1\n",
    "prob = normal_rv.cdf(1) - normal_rv.cdf(-1) \n",
    "print(f\"P(-1 <= X <= 1) = {prob}\") \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Similarities\n",
    "- Both **Confusion Matrix** and **ROC Curve** are used to evaluate classification models.\n",
    "- **Probability Distributions** like Bernoulli and Binomial are used for discrete events.\n",
    "- **Generative and Discriminative Models** both attempt to classify data but with different approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Differences\n",
    "- **Confusion Matrix** provides performance at a specific threshold, while **ROC Curve** shows performance across thresholds.\n",
    "- **Generative Models** focus on modeling data distribution (joint probability), while **Discriminative Models** focus on boundaries (conditional probability).\n",
    "- **Residuals** are used in regression analysis, whereas **confusion matrices** apply to classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
